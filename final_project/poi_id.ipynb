{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Fraud From Enron Email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nanodegree project for Intro to Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One - Understanding the Dataset and Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "To better understand the dataset, an exploration is performed here and the results are summarized as following,\n",
    "- there are 146 data points with 21 features, and a total of 3066 obervations.\n",
    "- there are 18 people who is an point of interest.\n",
    "- 1,358 data points are missing.\n",
    "- the top 3 features with most missing values are \"loan_advances\", \"director_fees\", and \"restricted_stock_deferred\".\n",
    "\n",
    "More detailed exploration and analysis are listed as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of data points\n",
    "len(data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of features available\n",
    "len(data_dict['METTS MARK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# available features\n",
    "data_dict[\"METTS MARK\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# people of interest\n",
    "count = 0\n",
    "for key, item in data_dict.iteritems():\n",
    "    if item[\"poi\"]:\n",
    "        print key\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a dictionary for all missing values\n",
    "missing = {}\n",
    "for key, item in data_dict.iteritems():\n",
    "    for elem, value in item.iteritems():\n",
    "        if value == \"NaN\":\n",
    "            if elem not in missing:\n",
    "                missing[elem] = 1\n",
    "            else:\n",
    "                missing[elem] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of missing values\n",
    "number_of_missing = 0\n",
    "for key, item in missing.iteritems():\n",
    "    number_of_missing += item\n",
    "number_of_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check who isn't missing the feature 'load_advances'\n",
    "# outputs the person's name and a boolean value indicated whether the person is a poi.\n",
    "for key, item in data_dict.iteritems():\n",
    "    if item[\"loan_advances\"] != \"NaN\":\n",
    "        print \"name: \", key, \",poi:\", item[\"poi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check who isn't missing the feature 'director_fees'\n",
    "# outputs the person's name and a boolean value indicated whether the person is a poi.\n",
    "for key, item in data_dict.iteritems():\n",
    "    if item[\"director_fees\"] != \"NaN\":\n",
    "        print \"name:\", key, \",poi:\", item[\"poi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check who isn't missing the feature 'restricted_stock_deferred'\n",
    "# outputs the person's name and a boolean value indicated whether the person is a poi.\n",
    "for key, item in data_dict.iteritems():\n",
    "    if item[\"restricted_stock_deferred\"] != \"NaN\":\n",
    "        print \"name\", key, \",poi:\", item[\"poi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check who isn't missing the feature 'deferral_payments'\n",
    "# outputs the person's name and a boolean value indicated whether the person is a poi.\n",
    "for key, item in data_dict.iteritems():\n",
    "    if item[\"deferral_payments\"] != \"NaN\":\n",
    "        print \"name:\", key, \",poi:\", item[\"poi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, it doesn't seem to have a clear pattern on whether a poi is missing a value or not. The investigation on missing values ends here, and the missing values will be replaced with '0' after feature formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Two - Outlier Investigation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the TOTAL Data Point\n",
    "As we already known in mini projects, there is an outlier named \"TOTAL\" in this dataset. We will need to remove it before any further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the outlier 'TOTAL'\n",
    "data_dict.pop(\"TOTAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots of the Outliers\n",
    "\n",
    "To understand the outliers in this dataset, plots are created by using salary against every other feature but poi, which is used to color data points in each plot. As a starting point, all the available features will be selected and put into the model. Later in this report, some features will be removed based on their PCA importance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create features for plots\n",
    "# features_list is a list of strings, each of which is a feature name.\n",
    "# The first feature must be \"poi\".\n",
    "features_list = ['poi',\n",
    "                 'salary',\n",
    "                 'to_messages',\n",
    "                 'deferral_payments',\n",
    "                 'total_payments',\n",
    "                 'exercised_stock_options',\n",
    "                 'bonus',\n",
    "                 'restricted_stock',\n",
    "                 'shared_receipt_with_poi',\n",
    "                 'restricted_stock_deferred',\n",
    "                 'total_stock_value',\n",
    "                 'expenses',\n",
    "                 'loan_advances',\n",
    "                 'from_messages',\n",
    "                 'other',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'director_fees',\n",
    "                 'deferred_income',\n",
    "                 'long_term_incentive',\n",
    "                 'from_poi_to_this_person']\n",
    "\n",
    "# format the dataset\n",
    "data = featureFormat(data_dict, features_list)\n",
    "\n",
    "# create a pandas dataframe\n",
    "df = pd.DataFrame(data, columns = features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plots, blue color stands for poi, red color stands for non-poi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ggplot import *\n",
    "\n",
    "# iter through all features\n",
    "# x axis will always be salary\n",
    "# poi is represented by colors of points\n",
    "# the rest of features are put in y axis\n",
    "for feature in features_list:\n",
    "    if feature != \"poi\" and feature != \"salary\":\n",
    "        print ggplot(aes(x = 'salary', y = feature, color = 'poi'),\n",
    "               data = df) +\\\n",
    "        geom_point() +\\\n",
    "        ggtitle(\"salary against \" + feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of removing outliers is to prevent the model being misrepresented by extreme cases, which comes with an assumption that either the extreme cases rarely happen, or they don't carry engough valuable infomration to be kept in the model. This can be true for some of the features, but could be controversy for \"total_payment\" feature, and shouldn't be applied to \"exercised_stock_options\" as the top four outliers are all person of interest. On the other hand, if we are to treat top 10% of each feature as outliers, it is not hard to imagine that the final dataset will have much less than 90%. A large deduction in the original dataset will cause the model beoming weaker.\n",
    "\n",
    "Given all these thoughts, a detailed outliers removal is performed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Outliers\n",
    "\n",
    "Instead of removing a certain percent of data points by each features directly, as a starting point, we will fit a dummy linear regression model, calculate the deviations, and treat data points with the highest deviations as outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, prepare features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only want to remove the outliers by getting deviations from a linear model, there's no need to split the dataset as for now. The model is built as following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(features, labels)\n",
    "predictions = reg.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### check the score\n",
    "reg.score(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the cleaner function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def outlierCleaner(predictions, features, labels):\n",
    "    \"\"\"\n",
    "        clean away the 10% of points that have the largest\n",
    "        residual errors (different between the prediction\n",
    "        and the actual label value)\n",
    "\n",
    "        return two lists - normals and outliers. Outliers\n",
    "        are data points with the top 10% largest residual\n",
    "        errors, the rest are in normals. Both of the lists\n",
    "        are formatted as numpy array, and exactly like the\n",
    "        formats after calling featureFormat.\n",
    "    \"\"\"\n",
    "\n",
    "    normals = []\n",
    "    outliers = []\n",
    "    data = []\n",
    "    length = int(len(predictions) * 0.9) + 1 # define the number of data points to be kept in normals\n",
    "\n",
    "    ### create a dataset with a format:\n",
    "    ### tuple(feature, label, residual errors)\n",
    "    for i in range(len(predictions)):\n",
    "        result = features[i], labels[i], (labels[i] - predictions[i]) ** 2\n",
    "        data.append(tuple(result))\n",
    "        \n",
    "    ### sort dataset by deviations\n",
    "    data.sort(key=lambda value: value[2])\n",
    "\n",
    "    ### access dataset and create normals and outliers\n",
    "    count = 0\n",
    "    for values in data:\n",
    "        count += 1\n",
    "        if count <= length:\n",
    "            normals.append(np.append([values[1]],values[0]))\n",
    "        else:\n",
    "            outliers.append(np.append([values[1]],values[0]))\n",
    "    \n",
    "    return normals, outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### extract normal data points and outliers\n",
    "cleaned_data, outliers = outlierCleaner(predictions, features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### extract labels and features from cleaned_data\n",
    "cleaned_labels, cleaned_features = targetFeatureSplit(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit the model again\n",
    "reg.fit(cleaned_features, cleaned_labels)\n",
    "# check the score\n",
    "reg.score(cleaned_features, cleaned_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A removal of the outliers improved the score of the linear model dramastically from 0.35 to 0.82. Although it's good to see improvement in score, it's always necessary to take a look at the removed outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Outliers\n",
    "\n",
    "Although the featureFormat function creates a convenience here by turning a python dictionary into a numpy array, it also creates a difficulty checking who's been removed, as it loses information on keys of the dictionary after processing it. Moreover, the final test for this project uses my_dataset as an input, which is a python dicionary. If the removal of outliers do not happen on my_dataset, the test won't reflect the cleaning effort. Therefore, it's necessay to reformat the numpy array into a python dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featureReformat(numpy_array, features):\n",
    "    \"\"\"\n",
    "        Format a numpy array object into a python\n",
    "        dictionary object.\n",
    "        \n",
    "        Take a numpy array and features as inputs and\n",
    "        return a python dictionary using features as\n",
    "        keys and numpy array as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for array in numpy_array:\n",
    "        data_point = {}\n",
    "        for i in range(len(features)):\n",
    "            value = array[i]\n",
    "            key = features[i]\n",
    "            data_point[key] = value\n",
    "        result.append(data_point)\n",
    "\n",
    "    return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def personMapping(dict_list, dataset):\n",
    "    \"\"\"\n",
    "        Mapping a person's name based on the values of\n",
    "        features.\n",
    "        \n",
    "        Take a list of dictionaries that has all the values\n",
    "        of person's features, and map it with a dataset\n",
    "        which has a person's name as a key, and its features\n",
    "        and values as the key's item.\n",
    "        \n",
    "        Return a dictionary with a person's name as its key,\n",
    "        and another dictionary as its value, which has features\n",
    "        as its key, and values of features as its values,\n",
    "        {name_of_person_1:\n",
    "            {feature_1: value,\n",
    "             feature_2: value,\n",
    "             feature_3: value,\n",
    "             ...},\n",
    "         name_of_person_2:\n",
    "             {...}}\n",
    "    \"\"\"\n",
    "    \n",
    "    my_dataset = {}\n",
    "    \n",
    "    ### iter through the dataset\n",
    "    for key, item in dataset.iteritems():\n",
    "        \n",
    "        ### open the dictionary list\n",
    "        for data in dict_list:\n",
    "            \n",
    "            ### open the features list\n",
    "            for feature in features_list:\n",
    "                \n",
    "                ### filter out 'NaN' in the dataset\n",
    "                ### check all the '0' values\n",
    "                if item[feature] == \"NaN\":\n",
    "                    if int(data[feature]) == 0:\n",
    "                        find = True\n",
    "                    else:\n",
    "                        find = False\n",
    "                        break\n",
    "                \n",
    "                else:\n",
    "                    ### check every other feature between dictionary list and dataset\n",
    "                    ### using a logical value 'find' to determine if a match is found\n",
    "                    if int(data[feature]) == item[feature]:\n",
    "                        find = True\n",
    "                    else:\n",
    "                        find = False\n",
    "                        break\n",
    "            \n",
    "            ### iter through all features once\n",
    "            ### if found, map the data to my_dataset\n",
    "            if find:\n",
    "                my_dataset[key] = item\n",
    "                \n",
    "    return my_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Outliers\n",
    "\n",
    "There are 15 people are identified as outliers, among which only 2 of them are non-person of interest. Given the fact that there are 18 person of interest, and 14 of them showed in the outliers, there might be an issue if these outliers are cleaned away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outliers_dataset = personMapping(featureReformat(outliers, features_list), data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(outliers_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, item in outliers_dataset.iteritems():\n",
    "    if item['poi'] == 0.0:\n",
    "        print key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New Datasets\n",
    "\n",
    "As mentioned above, simply removing the outliers might cause an issue for later on analysis. While the imporvement in score of the linear model is surely tempting, do note that, this is not the model that we will use to conduct machine learning in this dataset. However, since we don't want to miss any possible improvements in our future models, we will use both datasets in later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_full_dataset = data_dict\n",
    "my_cleaned_dataset = personMapping(featureReformat(cleaned_data, features_list), data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featureLabelSplit(my_dataset, features_list):\n",
    "    \"\"\"\n",
    "        A simple function creates features and labels\n",
    "        \n",
    "        Return features and labels\n",
    "    \"\"\"\n",
    "    data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, there's no surprising that most of the person of interest might be flagged as outliers given the background knowledge of Enron Fraud. In this case, the outliers are the targets we want to find, according to <a href='https://discussions.udacity.com/t/outlier-removal/7446' target='_blank'>this post in discussion forum</a>, we can manually decided to include or exclude the outliers or not in the training set. This strategy will be applied when processing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Three - Optimize Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New Features\n",
    "\n",
    "To dig out more patterns from the dataset, three new features, \"stock_salary_ratio\", \"poi_from_ratio\", \"poi_to_ratio\", are created as following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature stock_salary_ratio\n",
    "\n",
    "stock_salary_ratio takes the result from total_stock_value divided by salary. This feature is useful based on the assumption that a person of interest usually has a unusual large stock value since it's under the table, while salary information could be more easily known by public, thus the ratio could give information to identify the poi. The bigger the ratio, the more likely it is a poi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature poi_from_ratio and poi_to_ratio\n",
    "poi_from_ratio takes result from from_poi_to_this_person divided by from_messages. This feature assumes that if a person is a poi, he/she tends to have more contacts with another poi, therefore the ratio would be bigger. And same applie to feature poi_to_ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### add new features to dataset\n",
    "for key, item in data_dict.iteritems():\n",
    "    ### add stock_salary_ratio\n",
    "    if item['salary'] != \"NaN\" and item['total_stock_value'] != \"NaN\":\n",
    "        item['stock_salary_ratio'] = float(item['total_stock_value']) / item['salary']\n",
    "    else:\n",
    "        item['stock_salary_ratio'] = \"NaN\"\n",
    "    \n",
    "    ### add poi_from_ratio\n",
    "    if item['from_messages'] != \"NaN\" and item['from_poi_to_this_person'] != \"NaN\":\n",
    "        item['poi_from_ratio'] = float(item['from_poi_to_this_person']) / item['from_messages']\n",
    "    else:\n",
    "        item['poi_from_ratio'] = \"NaN\"\n",
    "        \n",
    "    ### add poi_to_ratio\n",
    "    if item[\"to_messages\"] != \"NaN\" and item[\"from_this_person_to_poi\"] != \"NaN\":\n",
    "        item[\"poi_to_ratio\"] = float(item[\"from_this_person_to_poi\"]) / item[\"to_messages\"]\n",
    "    else:\n",
    "        item[\"poi_to_ratio\"] = \"NaN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### update dataset\n",
    "my_full_dataset = data_dict\n",
    "my_cleaned_dataset = personMapping(featureReformat(cleaned_data, features_list), data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### update features_list\n",
    "features_list += [\"stock_salary_ratio\", \"poi_from_ratio\", \"poi_to_ratio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Depending on the algorithms chosen, feature scaling may be necessary. We will perform feature scaling anyway in case it is needed for later algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(data_dict, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "features = min_max_scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "To get a comprehensive processing before any fitting into models, a variety of feature selection methods for classification listed in \n",
    "<a href='http://scikit-learn.org/stable/modules/feature_selection.html' target='_blank'>sklearn documentations</a> \n",
    "are explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "k_best = SelectKBest(k = 5)\n",
    "k_best_features = k_best.fit_transform(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_best_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_best_result = zip(features_list[1:], k_best.scores_, k_best.get_support())\n",
    "k_best_result.sort(key=lambda value:value[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svc = LinearSVC(penalty=\"l1\", dual=False, random_state=31)\n",
    "svc_features = svc.fit_transform(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc_result = zip(features_list[1:], svc.coef_[0])\n",
    "svc_result.sort(key=lambda value:value[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RandomizedLogisticRegression\n",
    "randomized_logistic = RandomizedLogisticRegression(C=1, selection_threshold=0.01, random_state=31)\n",
    "randomized_features = randomized_logistic.fit_transform(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randomized_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randomized_result = zip(features_list[1:], randomized_logistic.scores_, randomized_logistic.get_support())\n",
    "randomized_result.sort(key=lambda value:value[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "tree = ExtraTreesClassifier(max_features=5, random_state=31)\n",
    "tree_features = tree.fit_transform(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree_result = zip(features_list[1:], tree.feature_importances_)\n",
    "tree_result.sort(key=lambda value: value[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=5)\n",
    "pca_features = pca.fit_transform(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_result = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Among Feature Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randomized_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize Feature Selection and More\n",
    "\n",
    "Till this point, no models have been fitted and no results can be used to determine which feature selection method would work for the best. In order to find the best method, all four feature selection algorithms will be tested, so does a combination of the four.\n",
    "\n",
    "Also keep in mind that, the above analysis was done based on the whole dataset, not the cleaned dataset without outliers. Therefore, to automize the pipeline in the later report, a list of feature selection functions and a function to combine all the features are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_selection = [('k_best', SelectKBest(k = 5)),\n",
    "                     ('linear_svc_l1', LinearSVC(C=0.01, penalty=\"l1\", dual=False, random_state=31)),\n",
    "                     ('logistic_reg', RandomizedLogisticRegression(C=1, selection_threshold=0.01, random_state=31)),\n",
    "                     ('extra_tree', ExtraTreesClassifier(max_features=5, random_state=31))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "\n",
    "### combine pca to feature selection\n",
    "combined_feature = []\n",
    "for method in feature_selection:\n",
    "    new_method = FeatureUnion(('pca', PCA(n_components=5)), method)\n",
    "    name = method[0] + \" with pca\"\n",
    "    combined_feature.append((name, new_method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### update feature selection list\n",
    "feature_selection += combined_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although four different feature selection methods are chosen, their parameters could be tuned and a potential better result could be found too. But it will be left to future. On the other hand, a combination of the feature selection results is really a conservative way, and it's doubtful it would work. Anyway, we will let the score speak of itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Four - Pick and Tune an Algorithm\n",
    "\n",
    "According to \n",
    "<a href='http://scikit-learn.org/stable/tutorial/machine_learning_map/' target='_blank'>this cheat sheet in sklearn</a>, \n",
    "there are at least four classification methods can be used,\n",
    "- LinearSVC\n",
    "- KNeighbors Classifier\n",
    "- SVC\n",
    "- Ensemble Classifers\n",
    "\n",
    "In this report, we will check on LinearSVC, KNeighborsClassifier, and AdaBoostClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LinearSVC\n",
    "\n",
    "Parameters to be tuned:\n",
    "- C: float, default = 1.0\n",
    "- loss: \"hinge\" or \"squared_hinge\", default = \"squared_hinge\"\n",
    "- penalty: \"l1\" or \"l2\", default = \"l2\"\n",
    "- dual: boolean, prefer False when n_samples > n_features, default = True\n",
    "- tol: float, default = 0.0001\n",
    "- max_iter: integer, default = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_svc = LinearSVC(random_state=31, dual=False)\n",
    "\n",
    "params_svc = {'linear_svc__C':[1e-2, 1e-1, 1, 1e2, 1e3],\n",
    "              'linear_svc__penalty': [\"l1\", \"l2\"],\n",
    "              'linear_svc__tol': [1e4, 1e3, 1e2, 1],\n",
    "              'linear_svc__max_iter': [1e2, 1e3, 1e4, 1e5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier\n",
    "\n",
    "Parameters to be tuned:\n",
    "- n_neighbors: integer, default = 5\n",
    "- weights: \"uniform\" or \"distance\", default = \"uniform\"\n",
    "- algorithm: \"auto\", \"ball_tree\", \"kd_tree\" or \"brute\", default = \"auto\"\n",
    "- leaf_size: integer, default = 30\n",
    "- p, integer, 1 for manhattan_distance, 2 for euclidean_distance, default = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k_neighbors = KNeighborsClassifier()\n",
    "\n",
    "params_kneighbors = {'k_neighbors__n_neighbors': [1, 5, 10, 20, 50],\n",
    "                     'k_neighbors__weights': [\"uniform\", \"distance\"],\n",
    "                     'k_neighbors__algorithm':[\"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "                     'k_neighbors__leaf_size': [2, 5, 10, 30, 50, 100],\n",
    "                     'k_neighbors__p': [1,2]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoostClassifer\n",
    "\n",
    "Parameters to be tuned:\n",
    "- n_estimators: integer, default = 50\n",
    "- learning_rate: float, default = 1\n",
    "- algorithm: \"SAMME\" or \"SAMME.R\", default = \"SAMME.R\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_boost = AdaBoostClassifier(random_state=31)\n",
    "\n",
    "params_adaboost = {'ada_boost__base_estimator': [DecisionTreeClassifier(), None],\n",
    "                   'ada_boost__n_estimators': [1, 5, 10, 20, 50, 100],\n",
    "                   'ada_boost__algorithm': ['SAMME', 'SAMME.R'],\n",
    "                   'ada_boost__learning_rate': [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10],\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = [('linear_svc', linear_svc, params_svc),\n",
    "               ('k_neighbors', k_neighbors, params_kneighbors),\n",
    "               ('ada_boost', ada_boost, params_adaboost)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Five - Validate and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "To prevent overfitting, a cross validation is needed to split the dataset into training and testing. We will use train_test_split method with a default test_size of 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def trainTestSplit(my_dataset, features_list):\n",
    "    \"\"\"\n",
    "        A training and testing set split function.\n",
    "        \n",
    "        Take my_dataset and features_list as input, call on\n",
    "        featueLabelSplit to create features and labels. Then\n",
    "        use train_test_split to split datasets.\n",
    "        \n",
    "        Return training and testing datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    features, labels = featureLabelSplit(my_dataset, features_list)\n",
    "    \n",
    "    features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "        features, labels, test_size=0.25, random_state=42)\n",
    "    \n",
    "    return features_train, features_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def evaluateModel(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        A model evaluator.\n",
    "        Calculate the model's accuracy score, f1 score,\n",
    "        precision score, and recall score. \n",
    "        \n",
    "        Return nothing. Print out the scores as side effects.\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    print \"\"\"    Accuracy score: {}\n",
    "    F1 score: {}\n",
    "    Precision score: {}\n",
    "    Recall score: {}\"\"\".format(accuracy, f1, precision, recall)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Six - Find the Best Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "def tuneEstimator(pipeline, param, features_train, features_test, labels_train):\n",
    "    \"\"\"\n",
    "        Tune the classifiers to find the best estimator.\n",
    "        \n",
    "        Return the best estimator, predictions and scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    clf = GridSearchCV(pipeline, param)\n",
    "    \n",
    "    ### train the model\n",
    "    clf.fit(features_train, labels_train)\n",
    "                    \n",
    "    ### store the tuning results\n",
    "    tuned_scores = clf.grid_scores_\n",
    "                    \n",
    "    ### use the best estimator\n",
    "    best_clf = clf.best_estimator_\n",
    "    labels_pred = best_clf.predict(features_test)\n",
    "    \n",
    "    return best_clf, labels_pred, tuned_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainModel(my_dataset, features_list, feature_selection=feature_selection, classifiers=classifiers, scaling=False):\n",
    "    \"\"\"\n",
    "        A model training function.\n",
    "        \n",
    "        Take a dataset in python dictionary format, a list of\n",
    "        features, a list of feature selection methods, and a\n",
    "        list of classification methods. Iter through each list\n",
    "        and make combinations of different feature selection\n",
    "        method with different classification method. Then use \n",
    "        tuneEstimator to tune the model. Finally, it evaluates\n",
    "        the model based on accuracy score, precision score,\n",
    "        recall score, and f1 score.\n",
    "        \n",
    "        If scaling is True, it will scale features only when\n",
    "        appropriate classifiers are used. If scaling_all is True,\n",
    "        it will scale features for all classifiers.\n",
    "        \n",
    "        Return a list of models and tuned scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### split the training and testing sets\n",
    "    features_train, features_test, labels_train, labels_test = trainTestSplit(my_dataset, features_list)\n",
    "    \n",
    "    trained_model = []\n",
    "    count = 0\n",
    "    tuned_score = []\n",
    "    \n",
    "    ### iter through feature selection and classification methods\n",
    "    for selection_method in feature_selection:\n",
    "        for item in classifiers:\n",
    "            \n",
    "            count += 1\n",
    "            print \"Model {} \\n-working on classifier {}, using slection method {}\".format(count, item[0], selection_method[0])\n",
    "            \n",
    "            ### add a time function to calculate time used by each model\n",
    "            from time import time\n",
    "            t0 = time()\n",
    "            \n",
    "            ### unpack name, function and parameters\n",
    "            classifier = item[:2]\n",
    "            param = item[2]\n",
    "                \n",
    "            ### scale the features before training\n",
    "            if scaling:\n",
    "                features_train = MinMaxScaler().fit_transform(features_train)\n",
    "                features_test = MinMaxScaler().fit_transform(features_test)\n",
    "                \n",
    "            try:\n",
    "                \n",
    "                ### build pipeline\n",
    "                pipeline = Pipeline([selection_method, classifier[:2]])\n",
    "                \n",
    "                ### tune the model\n",
    "                try:\n",
    "\n",
    "                    print \"--start tuning...\"\n",
    "                    clf, labels_pred, grid_scores = tuneEstimator(pipeline, param, features_train, features_test, labels_train)\n",
    "                    \n",
    "                    ### store the tuning results\n",
    "                    tuned_score.append(grid_scores)\n",
    "                    \n",
    "                    ### store model's information, including name, function, and parameters\n",
    "                    model_name = item[0] + \" with \" + selection_method[0]\n",
    "                    model_info = (model_name, clf)\n",
    "                    trained_model.append(model_info)\n",
    "\n",
    "                    print \"--training on {} complete, time used {}\".format(model_name, time() - t0)\n",
    "\n",
    "                    ### print out evaluation scores\n",
    "                    evaluateModel(labels_test, labels_pred)\n",
    "\n",
    "                    print \"\"\n",
    "                \n",
    "                except Exception, e:\n",
    "                    print \"--error on tuning: \\n\", e, \"\\n\"\n",
    "                                \n",
    "            except Exception, e:\n",
    "                print \"-error on classifying: \\n\", e, \"\\n\"\n",
    "            \n",
    "    return trained_model, tuned_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Estimators on Full Dataset\n",
    "\n",
    "When runing the esimators on full dataset, there are 18 models generated. A comparison among the best choices is as following, listed as model number, feature selection method, classification method, accuracy score, F1 score, precision score, recall score, and time consumption.\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>Model No.</td>\n",
    "        <td>Feature Selection Method</td>\n",
    "        <td>Classification Method</td>\n",
    "        <td>Accuracy Score</td>\n",
    "        <td>F1 Score</td>\n",
    "        <td>Precision Score</td>\n",
    "        <td>Recall Score</td>\n",
    "        <td>Time Used (s)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>k_best</td>\n",
    "        <td>linear_svc</td>\n",
    "        <td>0.94</td>\n",
    "        <td>0.5</td>\n",
    "        <td>1.0</td>\n",
    "        <td>0.33</td>\n",
    "        <td>1.64</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>8</td>\n",
    "        <td>logistic_reg</td>\n",
    "        <td>k_neighbors</td>\n",
    "        <td>0.94</td>\n",
    "        <td>0.5</td>\n",
    "        <td>1.0</td>\n",
    "        <td>0.33</td>\n",
    "        <td>115.33</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>11</td>\n",
    "        <td>extra_tree</td>\n",
    "        <td>k_neighbors</td>\n",
    "        <td>0.94</td>\n",
    "        <td>0.5</td>\n",
    "        <td>1.0</td>\n",
    "        <td>0.33</td>\n",
    "        <td>22.03</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>14</td>\n",
    "        <td>pca</td>\n",
    "        <td>k_neighbors</td>\n",
    "        <td>0.94</td>\n",
    "        <td>0.5</td>\n",
    "        <td>1.0</td>\n",
    "        <td>0.33</td>\n",
    "        <td>3.59</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "As shown in the table, all the estimators have the same performance on scores, while the time consumption varies. Based on the results, LinearSVC with SelectKBest performs the best as it only takes 1.64 seconds, and the second best comes with KNeighborsClassifier and PCA. These two models will be used as candidates to build the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### scaled results\n",
    "full_model_sets, tuned_score_1 = trainModel(my_full_dataset, features_list, scaling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### non-scaled results\n",
    "#full_model_sets, tuned_score_1 = trainModel(my_full_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Improvement Among the Best Estimators\n",
    "Although the parameters for classifiers are tuned, parameters for feature selection methods might as well be tuned to to find better outcome. We will tune the parameters here and see if anything better could be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_kbest_linearsvc = full_model_sets[0][1]\n",
    "pipeline_pca_kneighbors = full_model_sets[13][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_k_best = {'k_best__k': [1,2,3,4,5,6,7,8,9,10,11,12]}\n",
    "\n",
    "param_pca = {'pca__n_components': [1,2,3,4,5,6,7,8,9,10,11,12]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = trainTestSplit(my_full_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kbest_linearsvc = GridSearchCV(pipeline_kbest_linearsvc, param_k_best)\n",
    "kbest_linearsvc.fit(features_train, labels_train)\n",
    "labels_pred = kbest_linearsvc.predict(features_test)\n",
    "evaluateModel(labels_test, labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_kneighbors = GridSearchCV(pipeline_pca_kneighbors, param_pca)\n",
    "pca_kneighbors.fit(features_train, labels_train)\n",
    "labels_pred = pca_kneighbors.predict(features_test)\n",
    "evaluateModel(labels_test, labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result turned out that no better solution was found for full dataset. The final estimator for full dataset is as below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_kbest_linearsvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run estimators on cleaned dataset.\n",
    "\n",
    "Similar to running estimators on full dataset, there are attempts to generate 18 models. However, as shown below, when using combined_feature as the feature selection method, it kept saying missing classes, a futher investigation is needed to figure this out. As for now, a summary table of potential choices for the final model is attached below,\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>Model No.</td>\n",
    "        <td>Feature Selection Method</td>\n",
    "        <td>Classification Method</td>\n",
    "        <td>Accuracy Score</td>\n",
    "        <td>F1 Score</td>\n",
    "        <td>Precision Score</td>\n",
    "        <td>Recall Score</td>\n",
    "        <td>Time Used (s)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>k_best</td>\n",
    "        <td>ada_boost</td>\n",
    "        <td>0.97</td>\n",
    "        <td>0.67</td>\n",
    "        <td>1.0</td>\n",
    "        <td>0.5</td>\n",
    "        <td>2.52</td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>10</td>\n",
    "        <td>extra_tree</td>\n",
    "        <td>linear_svc</td>\n",
    "        <td>0.97</td>\n",
    "        <td>0.67</td>\n",
    "        <td>1.0</td>\n",
    "        <td>0.5</td>\n",
    "        <td>6.38</td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>12</td>\n",
    "        <td>extra_tree</td>\n",
    "        <td>ada_boost</td>\n",
    "        <td>0.97</td>\n",
    "        <td>0.67</td>\n",
    "        <td>1.0</td>\n",
    "        <td>0.5</td>\n",
    "        <td>10.56</td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>13</td>\n",
    "        <td>pca</td>\n",
    "        <td>linear_svc</td>\n",
    "        <td>0.97</td>\n",
    "        <td>0.67</td>\n",
    "        <td>1.0</td>\n",
    "        <td>0.5</td>\n",
    "        <td>1.25</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "It's interesting that when runing on cleaned datasets, the performance in general is much better. For the final analysis, model 3 and 13 are chosen as candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### scaled results\n",
    "cleaned_model_sets, tuned_score_2 = trainModel(my_cleaned_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### non-scaled results\n",
    "cleaned_model_sets, tuned_score_2 = trainModel(my_cleaned_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_kbest_adaboost = cleaned_model_sets[2][1]\n",
    "pipeline_pca_linearsvc = cleaned_model_sets[9][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = trainTestSplit(my_cleaned_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kbest_adaboost = GridSearchCV(pipeline_kbest_adaboost, param_k_best)\n",
    "kbest_adaboost.fit(features_train, labels_train)\n",
    "clf = kbest_adaboost.best_estimator_\n",
    "labels_pred = clf.predict(features_test)\n",
    "evaluateModel(labels_test, labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_linearsvc = GridSearchCV(pipeline_pca_linearsvc, param_pca)\n",
    "pca_linearsvc.fit(features_train, labels_train)\n",
    "clf = pca_linearsvc.best_estimator_\n",
    "labels_pred = clf.predict(features_test)\n",
    "evaluateModel(labels_test, labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result turned out to be that the tuned result became even wrose. We will LinearSVC with PCA as the final estimator for cleaned dataset. The parameters are as below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_pca_linearsvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Seven - The Final Solution\n",
    "After comparing among feature selection methods, classification methods, carefully tuning parameters for the methods, and working on both the full dataset and the outlier-cleaned dataset, the best and fastest model turned out to be outlier-cleaned dataset with PCA as feature selection processor and LinearSVC as classification method. Parameters are as following,\n",
    "- PCA, n_components = 5.\n",
    "- LinearSVC, C = 0.1, dual = False, penalty = 'l1', tol = 1, max_iter = 100.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### prepare for the test\n",
    "clf = pipeline_pca_linearsvc\n",
    "my_dataset = my_cleaned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### find the explained variance ratio by PCA\n",
    "pca = clf.steps[0][1]\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### dump for testing\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "limitations\n",
    "\n",
    "different number of parameters tuned in each algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
