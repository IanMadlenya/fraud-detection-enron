{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Fraud From Enron Email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nanodegree project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from pprint import pprint\n",
    "import ggplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One - Understanding the Dataset and Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of features available\n",
    "len(data_dict['METTS MARK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['salary',\n",
       " 'to_messages',\n",
       " 'deferral_payments',\n",
       " 'total_payments',\n",
       " 'exercised_stock_options',\n",
       " 'bonus',\n",
       " 'restricted_stock',\n",
       " 'shared_receipt_with_poi',\n",
       " 'restricted_stock_deferred',\n",
       " 'total_stock_value',\n",
       " 'expenses',\n",
       " 'loan_advances',\n",
       " 'from_messages',\n",
       " 'other',\n",
       " 'from_this_person_to_poi',\n",
       " 'poi',\n",
       " 'director_fees',\n",
       " 'deferred_income',\n",
       " 'long_term_incentive',\n",
       " 'email_address',\n",
       " 'from_poi_to_this_person']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# available features\n",
    "data_dict[\"METTS MARK\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Features\n",
    "\n",
    "- salary: integer, salary when working in Enron.\n",
    "- to_messages: integer, number of emails sent.\n",
    "- deferral_payments: integer, the payment occured and would be paid back in the future.\n",
    "- total_payments: integer, the total payment when working in Enron.\n",
    "- exercised_stock_options: integer, the stock value that has been sold.\n",
    "- bonus: integer, bonus got when working in Enron.\n",
    "- restricted_stock: the stock value not fully transferable until certain conditions been met.\n",
    "- shared_receipt_with_poi: \n",
    "- restricted_stock_deferred:\n",
    "- total_stock_value:\n",
    "- expenses:\n",
    "- loan_advences:\n",
    "- from_messages:\n",
    "- other:\n",
    "- from_this_person_to_poi:\n",
    "- poi:\n",
    "- director_fees:\n",
    "- deferred_income:\n",
    "- long_term_incentive:\n",
    "- email_address:\n",
    "- from_poi_to_this_person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HANNON KEVIN P\n",
      "COLWELL WESLEY\n",
      "RIEKER PAULA H\n",
      "KOPPER MICHAEL J\n",
      "SHELBY REX\n",
      "DELAINEY DAVID W\n",
      "LAY KENNETH L\n",
      "BOWEN JR RAYMOND M\n",
      "BELDEN TIMOTHY N\n",
      "FASTOW ANDREW S\n",
      "CALGER CHRISTOPHER F\n",
      "RICE KENNETH D\n",
      "SKILLING JEFFREY K\n",
      "YEAGER F SCOTT\n",
      "HIRKO JOSEPH\n",
      "KOENIG MARK E\n",
      "CAUSEY RICHARD A\n",
      "GLISAN JR BEN F\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# people of interest\n",
    "count = 0\n",
    "for item, value in data_dict.iteritems():\n",
    "    if value[\"poi\"]:\n",
    "        print item\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Select Features\n",
    "As a starting point, all the available features will be selected and put into the model. Later in this report, some features will be removed based on their PCA importance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi',\n",
    "                 'salary',\n",
    "                 'to_messages',\n",
    "                 'deferral_payments',\n",
    "                 'total_payments',\n",
    "                 'exercised_stock_options',\n",
    "                 'bonus',\n",
    "                 'restricted_stock',\n",
    "                 'shared_receipt_with_poi',\n",
    "                 'restricted_stock_deferred',\n",
    "                 'total_stock_value',\n",
    "                 'expenses',\n",
    "                 'loan_advances',\n",
    "                 'from_messages',\n",
    "                 'other',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'poi',\n",
    "                 'director_fees',\n",
    "                 'deferred_income',\n",
    "                 'long_term_incentive',\n",
    "                 'from_poi_to_this_person']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def outlierCleaner(predictions, ages, net_worths):\n",
    "    \"\"\"\n",
    "        clean away the 10% of points that have the largest\n",
    "        residual errors (different between the prediction\n",
    "        and the actual net worth)\n",
    "\n",
    "        return a list of tuples named cleaned_data where\n",
    "        each tuple is of the form (age, net_worth, error)\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_data = []\n",
    "\n",
    "    length = int(len(predictions) * 0.9)\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        result = ages[i], net_worths[i], (net_worths[i] - predictions[i]) ** 2\n",
    "        cleaned_data.append(tuple(result))\n",
    "\n",
    "    cleaned_data.sort(key=lambda value: value[2])\n",
    "\n",
    "    cleaned_data = cleaned_data[: length]\n",
    "    print len(cleaned_data)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "outlier_cleaner = EllipticEnvelope(contamination = 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two - Optimize Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three - Pick and Tune an Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Four - Validate and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Create new feature(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "\n",
    "n_samples = 1000\n",
    "n_outliers = 50\n",
    "\n",
    "\n",
    "X, y, coef = datasets.make_regression(n_samples=n_samples, n_features=1,\n",
    "                                      n_informative=1, noise=10,\n",
    "                                      coef=True, random_state=0)\n",
    "\n",
    "# Add outlier data\n",
    "np.random.seed(0)\n",
    "X[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1))\n",
    "y[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)\n",
    "\n",
    "# Fit line using all data\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Robustly fit linear model with RANSAC algorithm\n",
    "model_ransac = linear_model.RANSACRegressor(linear_model.LinearRegression())\n",
    "model_ransac.fit(X, y)\n",
    "inlier_mask = model_ransac.inlier_mask_\n",
    "outlier_mask = np.logical_not(inlier_mask)\n",
    "\n",
    "# Predict data of estimated models\n",
    "line_X = np.arange(-5, 5)\n",
    "line_y = model.predict(line_X[:, np.newaxis])\n",
    "line_y_ransac = model_ransac.predict(line_X[:, np.newaxis])\n",
    "\n",
    "# Compare estimated coefficients\n",
    "print(\"Estimated coefficients (true, normal, RANSAC):\")\n",
    "print(coef, model.coef_, model_ransac.estimator_.coef_)\n",
    "\n",
    "plt.plot(X[inlier_mask], y[inlier_mask], '.g', label='Inliers')\n",
    "plt.plot(X[outlier_mask], y[outlier_mask], '.r', label='Outliers')\n",
    "plt.plot(line_X, line_y, '-k', label='Linear regressor')\n",
    "plt.plot(line_X, line_y_ransac, '-b', label='RANSAC regressor')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Try a varity of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Tune your classifier to achieve better than .3 precision and recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Dump your classifier, dataset, and features_list so anyone can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
