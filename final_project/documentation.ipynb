{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to identify whether a person is guilty for the notorious Enron Fraud, using publicly available information like finacial incomes and emails. \n",
    "\n",
    "From oil-trading, bandwidth trading, and weather trading with market-to-market accounting, Enron has moved its hands to a variety of commodities, got in touch with politicians like George W. Bush, and caused a great loss to the public, including the California electricity shortage. All these information can be useful if text learning was applied, and certain patterns could be found out like, a pattern indicating a decision-making person could very likely be a person of interest. However, this is not applied in this analysis since it's a more advanced topic.\n",
    "\n",
    "This analysis used a finacial dataset containing people's salary, stock information, and so on. During Enron Fraud, people like Jefferey Skilling, Key Lay, and Fastow all have dumped large amounts of stock options, and they are all guilty. This information can be very helpful to check on other person of interest, and can be easily refelected in the dataset. This is also where machine learning comes into play. By creating models which calculate relationships between a person of interest and its available quantitative data, machine learning tries to find and memorize a pattern that helps us identify a guilty person in the future.\n",
    "\n",
    "There are certain outliers by each feature. To find out the outliers, a multivariable linear regression model was created. By removing data points with top 10% variance between predicted and real values, an outlier-cleaned dataset was built for the analysis. However, as there were lots person of interest appearing in the outliers, the original dataset was kept too. The analysis later on performed investigation on two datasets, and trained estimators on both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, f you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five features in the final analysis after applying SelectKBest as feature selection.\n",
    "- exercised_stock_options, with a score of 319.54\n",
    "- total_stock_value, with a score of 211.70\n",
    "- bonus, with a score of 63.12\n",
    "- total_payments, 53.61\n",
    "- loan_advances, 46.03\n",
    "\n",
    "The feature selection method is picked after comparing to other three different feature selection methods, and performed with and without PCA. Feature scaling is used as it gives the fastest performance with highest scores, although there are good results without using scaling. The reason behind scaling is that, the huge difference in values between features like 'total_payments' and 'from_messages' or 'to_messages'. As features are not in the same scale, it's better to perform feature scaling before further analysis.\n",
    "\n",
    "Three new features, \"stock_salary_ratio\", \"poi_from_ratio\", \"poi_to_ratio\", are created when analyzing the data.\n",
    "\n",
    "stock_salary_ratio takes the result from total_stock_value divided by salary. This feature is useful based on the assumption that a person of interest usually has a unusual large stock value since it's under the table, while salary information could be more easily known by public, thus the ratio could give information to identify the poi. The bigger the ratio, the more likely it is a poi.\n",
    "\n",
    "poi_from_ratio takes result from from_poi_to_this_person divided by from_messages. This feature assumes that if a person is a poi, he/she tends to have more contacts with another poi, therefore the ratio would be bigger. And same applie to feature poi_to_ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final algorithm used is LinearSVC. There were three algorithms being picked and evaluated, and LinearSVC gave the fastest performance on the same scores. Part of the performance is attached as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned</th>\n",
       "      <th>model</th>\n",
       "      <th>scaled</th>\n",
       "      <th>feature_selection_method</th>\n",
       "      <th>classification_method</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>time_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>k_best</td>\n",
       "      <td>linear_svc</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333</td>\n",
       "      <td>1.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>k_best</td>\n",
       "      <td>linear_svc</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333</td>\n",
       "      <td>1.474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>k_best_with_pca</td>\n",
       "      <td>k_neighbors</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333</td>\n",
       "      <td>5.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "      <td>False</td>\n",
       "      <td>linear_svc_l1_with_pca</td>\n",
       "      <td>k_neighbors</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333</td>\n",
       "      <td>10.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>extra_tree</td>\n",
       "      <td>k_neighbors</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333</td>\n",
       "      <td>15.624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>23</td>\n",
       "      <td>False</td>\n",
       "      <td>extra_tree_with_pca</td>\n",
       "      <td>k_neighbors</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333</td>\n",
       "      <td>16.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>logistic_reg_with_pca</td>\n",
       "      <td>k_neighbors</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333</td>\n",
       "      <td>109.164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>logistic_reg</td>\n",
       "      <td>k_neighbors</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333</td>\n",
       "      <td>443.944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>k_best</td>\n",
       "      <td>linear_svc</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>k_best_with_pca</td>\n",
       "      <td>linear_svc</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>extra_tree</td>\n",
       "      <td>linear_svc</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>6.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>k_best</td>\n",
       "      <td>ada_boost</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>k_best_with_pca</td>\n",
       "      <td>ada_boost</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>3.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>extra_tree</td>\n",
       "      <td>linear_svc</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>6.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>True</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "      <td>extra_tree_with_pca</td>\n",
       "      <td>linear_svc</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>7.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>extra_tree</td>\n",
       "      <td>ada_boost</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>9.987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>extra_tree_with_pca</td>\n",
       "      <td>ada_boost</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>10.917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cleaned  model scaled feature_selection_method classification_method  \\\n",
       "0    False      1   True                   k_best            linear_svc   \n",
       "1    False      1  False                   k_best            linear_svc   \n",
       "2    False     14  False          k_best_with_pca           k_neighbors   \n",
       "3    False     17  False   linear_svc_l1_with_pca           k_neighbors   \n",
       "4    False     11  False               extra_tree           k_neighbors   \n",
       "5    False     23  False      extra_tree_with_pca           k_neighbors   \n",
       "6    False     20  False    logistic_reg_with_pca           k_neighbors   \n",
       "7    False      8  False             logistic_reg           k_neighbors   \n",
       "8     True      1   True                   k_best            linear_svc   \n",
       "9     True     13   True          k_best_with_pca            linear_svc   \n",
       "10    True     10   True               extra_tree            linear_svc   \n",
       "11    True      3  False                   k_best             ada_boost   \n",
       "12    True     15  False          k_best_with_pca             ada_boost   \n",
       "13    True     10  False               extra_tree            linear_svc   \n",
       "14    True     22  False      extra_tree_with_pca            linear_svc   \n",
       "15    True     12  False               extra_tree             ada_boost   \n",
       "16    True     24  False      extra_tree_with_pca             ada_boost   \n",
       "\n",
       "    accuracy_score  f1_score  precision_score  recall_score  time_used  \n",
       "0            0.944     0.500                1         0.333      1.424  \n",
       "1            0.944     0.500                1         0.333      1.474  \n",
       "2            0.944     0.500                1         0.333      5.054  \n",
       "3            0.944     0.500                1         0.333     10.154  \n",
       "4            0.944     0.500                1         0.333     15.624  \n",
       "5            0.944     0.500                1         0.333     16.920  \n",
       "6            0.944     0.500                1         0.333    109.164  \n",
       "7            0.944     0.500                1         0.333    443.944  \n",
       "8            0.970     0.667                1         0.500      1.506  \n",
       "9            0.970     0.667                1         0.500      1.969  \n",
       "10           0.970     0.667                1         0.500      6.446  \n",
       "11           0.970     0.667                1         0.500      2.590  \n",
       "12           0.970     0.667                1         0.500      3.621  \n",
       "13           0.970     0.667                1         0.500      6.520  \n",
       "14           0.970     0.667                1         0.500      7.133  \n",
       "15           0.970     0.667                1         0.500      9.987  \n",
       "16           0.970     0.667                1         0.500     10.917  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric item: “tune the algorithm”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the algorithm means to find a better solution specificly for the current problem. If an algorithm is not tuned well, it could lower down the accuracy score, percision and recall scores, while increasing the runtime.\n",
    "\n",
    "To tune the algorithm, a list of available parameters are put into a dictionary as keys, with a list of possible values as their values. Then GridSearchCV was conducted to search for the best solution among the given paramters. So the tuning result is limited by the range of values given to parameters, it is important to provide reasonable amount of values to be tuned while considering the time cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric item: “validation strategy”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without validation, the training might be lead to be overfitting. To avoid it, train_test_split was used to split the dataset into training and testing set. That is an easy and quick way to do cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The evaluation metrics in this analysis used are accuaracy score, f1 score, precision score and recall score.\n",
    "The final model has an accuracy score of 0.97, which means 97% of the predictions are found to be true.\n",
    "With a precision score of 1.0, it tells us that if this model predicts one person as a poi, then he is truly a poi. On the other hand, with a recall score of 0.5, only half of all the poi could be found out by this model.\n",
    "Finally, there's always a tradeoff between precision and recall, f1 comes and measure how well the tradeoff is. With a f1 score of 0.67, this model is fairly robust, while more improvement might be applied in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
